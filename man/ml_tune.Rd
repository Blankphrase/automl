% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ml_tune.R
\name{ml_tune}
\alias{ml_tune}
\title{Wrapper for auto-tune many ML algorithms supported by caret.}
\usage{
ml_tune(data, target, sampling = NULL, metric = "Accuracy",
  search = "random", k = 10, tuneLength = 2, repeats = 1,
  method = "xgbLinear", preProcess = NULL,
  summaryFunction = twoClassSummary, nthread = 3)
}
\arguments{
\item{data}{the data to be trained in dataframe format.}

\item{target}{A character, column name of the target variable.}

\item{sampling}{A character, examples are up, down, rose, smote as supported by caret. The current version also supports ADAS, ANS, BLSMOTE, DBSMOTE, RSLS, SLS.
For details on these sampling methods, please see the \href{smotefamily package}{https://CRAN.R-project.org/package=smotefamily} on CRAN.}

\item{metric}{A character, examples are Accuracy, Kappa, ROC,Sens, and Spec as natively supported in caret package. F measures are expected in version(0.1.1).}

\item{search}{A character, random or grid. Future version(0.1.1) would support user-defined hyper-parameter search.}

\item{k}{A numeric, the number of cross-validation folds.}

\item{tuneLength}{A numeric, the number of hyper-parameter combinations to try, the number of models to train is tuneLength\* k \* repeats.}

\item{repeats}{A numeric, the number of repeats in cross-validation.}

\item{method}{A character, the name of the machine learning algorithm.}

\item{preProcess}{A character vector, the names of the pre-processing methods to apply.}

\item{summaryFunction}{A function name. Use twoClassSummary for binary classification and multiClassSummary for multi-class classification.}

\item{nthread}{A numeric, the number of cores to use in model training. It is best to set it to the number of physical cores you have minus 1.}
}
\value{
a list contains the model informaiton. The same structure as train function in caret package would return.
}
\description{
Auto-tune ml model with different sampling methods, different metrics, preprocessing method, number of cores and etc, and return one model.
}
\details{
When using grid search, there will be N_hyper_params^tuneLength of the models being trained.
When using random grid search, there will be tuneLength of models being trained, plus the eta is not set.
Use Random whenever possible unless you what to fine-tune one machine learning algorithm.
}
\examples{

iris_classification=ml_tune(data=iris,target = "Species",metric = "Kappa",search = "random",k=5,tuneLength = 2,repeats = 1,
method = "rf",preProcess = c("center","scale"),summaryFunction = multiClassSummary)

predict(iris_classification,iris)

\dontrun{

 ml_tune(data=training,target="target",sampling="down",metric="Accuracy",search = "random"
 ,k=10,tuneLength=2,repeats=1,method="xgbLinear",preProcess=NULL,summaryFunction=twoClassSummary,nthread=4)
}



}
